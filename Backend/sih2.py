# -*- coding: utf-8 -*-
"""SIH2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JZR0ShJNiadr-y-A5NmYVIgd7jms4HaT
"""
import re
import nltk
from nltk import ne_chunk, pos_tag, word_tokenize
from nltk.chunk import tree2conlltags
import json          # chatBot
import requests
import fitz  # PyMuPDF Extract text from pdf.
import pytesseract
from PIL import Image
from sumy.parsers.plaintext import PlaintextParser  # it gives summary of text in input number of lines.
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer
import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

# import fitz  # PyMuPDF

# def extract_text_from_pdf(pdf_path):
#     text = ''
#     pdf_document = fitz.open(pdf_path)
#     num_pages = pdf_document.page_count

#     for page_number in range(num_pages):
#         page = pdf_document.load_page(page_number)
#         text += page.get_text()

#     pdf_document.close()
#     return text

# pdf_text2 = extract_text_from_pdf('/content/drive/MyDrive/SIH/policies/rulesandprocs/15.pdf')
# pdf_text2

def extract_text_from_pdf(pdf_path):
    text = ''
    pdf_document = fitz.open(pdf_path)
    
    num_pages = pdf_document.page_count

    for page_number in range(num_pages):
        page = pdf_document.load_page(page_number)
        images = page.get_images(full=True)  # Get all images in the page

        if images:
            for img_index, img_info in enumerate(images):
                xref = img_info[0]
                base_image = pdf_document.extract_image(xref)
                image_bytes = base_image["image"]

                # Save the image as a temporary file
                temp_image = f"temp_image_{page_number}_{img_index}.png"
                with open(temp_image, 'wb') as temp_file:
                    temp_file.write(image_bytes)

                # Perform OCR on the saved image file
                extracted_text = pytesseract.image_to_string(Image.open(temp_image))
                text += extracted_text + "\n"  # Add a new line after each image's text

                # Remove the temporary image file
                import os
                os.remove(temp_image)

        else:
            text += page.get_text()

    pdf_document.close()
    return text

# pdf_text = extract_text_from_pdf(r'C:\Users\progg\Desktop\desktop_p\DocuDeck\Scraper\policies\rulesandprocs\127.pdf')
# print(pdf_text)


token_access = "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"     # replace with your hugging face Token
headers = {"Authorization": f"Bearer {token_access}"}

API_URL = "https://api-inference.huggingface.co/models/rsvp-ai/bertserini-bert-base-squad"

def query(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()

# # Get user input for question and context
# user_question = input("Enter your question: ")
# user_context = pdf_text

# # Query the model with user inputs
# output = query({
#     "inputs": {
#         "question": user_question,
#         "context": user_context
#     },
# })

# print(output)


# Function to extract entities and key phrases
def extract_info(text):
    # Extract date, reference number, tender ID, and department using regex
    date_match = re.search(r'Dated (\d{2}/\d{2}/\d{4})', text)
    date = date_match.group(1) if date_match else None

    ref_number_match = re.search(r'No. ([\w/-]+)', text)
    ref_number = ref_number_match.group(1) if ref_number_match else None

    tender_id_match = re.search(r'Tender ID (\d{4}_\w+)', text)
    tender_id = tender_id_match.group(1) if tender_id_match else None

    department_match = re.search(r'Ministry of (.*?)\|', text)
    department = department_match.group(1).strip() if department_match else None

    # Tokenize the text and perform part-of-speech tagging
    words = word_tokenize(text)
    tagged_words = pos_tag(words)

    # Extract entities using NLTK's ne_chunk and tree2conlltags
    ne_tree = ne_chunk(tagged_words)
    named_entities = []
    for subtree in ne_tree:
        if isinstance(subtree, nltk.tree.Tree):
            entity = " ".join([token for token, pos, chunk in tree2conlltags(subtree)])
            named_entities.append(entity)

    # Extract key phrases (noun phrases)
    grammar = r"NP: {<DT>?<JJ>*<NN>}"
    cp = nltk.RegexpParser(grammar)
    phrases = cp.parse(tagged_words)
    noun_phrases = [' '.join(leaf[0] for leaf in tree.leaves()) for tree in phrases.subtrees() if tree.label() == 'NP']

    return date, ref_number, tender_id, department, named_entities[:10], noun_phrases[:10]

# Call the function to extract information
# date, ref_number, tender_id, department, named_entities, noun_phrases = extract_info(pdf_text)

# # Print the extracted information
# print(f"Date: {date}")
# print(f"Tender Reference Number: {ref_number}")
# print(f"Tender ID: {tender_id}")
# print(f"Department: {department}")
# print(f"Named Entities: {named_entities}")
# print(f"Noun Phrases: {noun_phrases}")
# # Sample text data (replace this with your actual text data)
# text_data = pdf_text

def generate_summary(text_data, num_sentences=5):
    tokenizer = Tokenizer("english")
    parser = PlaintextParser.from_string(text_data, tokenizer)
    
    summarizer = TextRankSummarizer()
    summary = summarizer(parser.document, num_sentences)
    
    print("The summary generated:")
    for sentence in summary:
        print(sentence)