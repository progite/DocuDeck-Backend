# -*- coding: utf-8 -*-
"""similar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m6UVu7OeYziMh_ZbGhzVhZft03L1zbmN
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os

from sumy.parsers.plaintext import PlaintextParser  # it gives summary of text in input number of lines.
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer

import fitz  # PyMuPDF Extract text from pdf.
import pytesseract
from PIL import Image
import re
import torch
from transformers import BertTokenizer, BertModel
from scipy.spatial.distance import cosine

# Load pre-trained model and tokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# Function to get BERT embeddings for a sentence
def get_bert_embeddings(sentence):
    inputs = tokenizer(sentence, return_tensors='pt', max_length=512, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()
    return embeddings

# Function to find similarity between a keyword and dictionary values based on BERT embeddings
def find_similarity_with_dict(keyword, my_dict):
    keyword_embedding = get_bert_embeddings(keyword)

    similarity_scores = {}
    for key in my_dict:
        value = my_dict[key]
    # for key, value in my_dict.items():
        value_embedding = get_bert_embeddings(value)
        similarity = 1 - cosine(keyword_embedding, value_embedding)
        similarity_scores[key] = similarity

    # Sort the dictionary by similarity scores and get the top 3 keys
    similar_keys = sorted(similarity_scores, key=similarity_scores.get, reverse=True)[:3]
    return similar_keys

def keyword_similarity(keywords, tags):
    matching_policy_ids = []
    for keyword in keywords:
        matching_policy_ids.extend(find_similarity_with_dict(keyword, tags))
    return matching_policy_ids
        
# def extract_text_from_pdf(pdf_path):
#     text = ''
#     pdf_document = fitz.open(pdf_path)
#     num_pages = pdf_document.page_count

#     for page_number in range(num_pages):
#         page = pdf_document.load_page(page_number)
#         images = page.get_images(full=True)  # Get all images in the page

#         if images:
#             for img_index, img_info in enumerate(images):
#                 xref = img_info[0]
#                 base_image = pdf_document.extract_image(xref)
#                 image_bytes = base_image["image"]

#                 # Save the image as a temporary file
#                 temp_image = f"temp_image_{page_number}_{img_index}.png"
#                 with open(temp_image, 'wb') as temp_file:
#                     temp_file.write(image_bytes)

#                 # Perform OCR on the saved image file
#                 extracted_text = pytesseract.image_to_string(Image.open(temp_image))
#                 text += extracted_text + "\n"  # Add a new line after each image's text

#                 # Remove the temporary image file
#                 import os
#                 os.remove(temp_image)

#         else:
#             text += page.get_text()

#     pdf_document.close()
#     return text, num_pages

def extract_text_from_pdf(pdf_path):
    text = ''
    pdf_document = fitz.open(pdf_path)
    num_pages = pdf_document.page_count

    for page_number in range(num_pages):
        page = pdf_document.load_page(page_number)
        images = page.get_images(full=True)  # Get all images in the page

        if images:
            for img_index, img_info in enumerate(images):
                xref = img_info[0]
                base_image = pdf_document.extract_image(xref)
                image_bytes = base_image["image"]

                # Save the image as a temporary file
                temp_image = f"temp_image_{page_number}_{img_index}.png"
                with open(temp_image, 'wb') as temp_file:
                    temp_file.write(image_bytes)

                # Perform OCR on the saved image file
                extracted_text = pytesseract.image_to_string(Image.open(temp_image))
                text += extracted_text + "\n"  # Add a new line after each image's text

                # Remove the temporary image file
                os.remove(temp_image)

        # Always perform OCR for text extraction
        extracted_text = page.get_text()
        text += extracted_text + "\n"  # Add a new line after each page's text

    pdf_document.close()
    return text, num_pages

def Summarize_doc(pdf_text, num_pages):
    # Initialize parser and tokenizer
    parser = PlaintextParser.from_string(pdf_text, Tokenizer("english"))

    # Initialize TextRank summarizer
    summarizer = TextRankSummarizer()

    # Generate summary
    return summarizer(parser.document, num_pages*10)
    summary = summarizer(parser.document, num_pages*10)  # Provide the number of sentences in the summary
    summary_sentences = [str(sentence) for sentence in summary]

# Convert summary sentences into a single string
    summary_text = ' '.join(summary_sentences)

    # Initialize TF-IDF vectorizer
    vectorizer = TfidfVectorizer()

    # Fit and transform the summary text into a TF-IDF vector representation
    summary_vectors = vectorizer.fit_transform([summary_text])

    # Get the TF-IDF matrix (sparse matrix in this case)
    tfidf_matrix = summary_vectors.toarray()

    return tfidf_matrix

def extract_text_between_keywords(paragraph, start_keyword, end_keyword, to_find):
    pattern = re.compile(f'{start_keyword}(.*?){end_keyword}', re.DOTALL | re.IGNORECASE)
    match = pattern.search(paragraph)

    if match:
        extracted_text = match.group(1).strip()
        # Remove ".pdf" from the extracted text
        if to_find == 'pdf':
            extracted_text = extracted_text.replace('.pdf', '')
        return extracted_text
    else:
        return "Pattern not found in the paragraph."

def find_tender_requirements(tender_path):
    start_keywords = ['Cover Details', 'Qualification']
    end_keywords = ['Tender Fee Details', 'Independent']
    to_find = ['pdf', 'eligibility']
    
    print("TENDER PATH", tender_path)
    pdf_text, num_pages = extract_text_from_pdf(tender_path)
    extracted = {}
    for idx in range(2):
        extracted[to_find[idx]] = extract_text_between_keywords(pdf_text, start_keywords[idx], end_keywords[idx], to_find[idx])    
    return extracted
# summary=Summarize_doc(pdf_text)

# Display the generated summary
# print({file_path},"Extractive Summary:")
# for sentence in summary:
#     print(sentence)


# # def convert_to_csv(summary, ):
#     import csv

# # Define the CSV file name
#     csv_filename = 'rules.csv'

#     # Append the sentences to a CSV file
#     with open(csv_filename, mode='a', newline='', encoding='utf-8') as file:
#         writer = csv.writer(file)
#         writer.writerow([file_path])
#         for sentence in summary:
#             writer.writerow([sentence])

# print(f"Sentences have been written to '{csv_filename}'")

def encode_sentences(sentences, vectorizer):
    # Transform sentences using TF-IDF vectorizer
    embeddings = vectorizer.transform(sentences).toarray()
    return embeddings

def compare_policy_rule_embeddings(embeddings_database1, embeddings_database2):
    # Compare policy embeddings from the first database with rule embeddings from the second database using cosine similarity
    print("[DEBUG], em_tender", embeddings_database1.shape[0], embeddings_database2.shape[0])
    similarity_matrix = cosine_similarity(embeddings_database1, embeddings_database2.reshape((1, 40)))
    return similarity_matrix

import pandas as pd
from sklearn.metrics import confusion_matrix
from difflib import SequenceMatcher

# Function to calculate similarity score using SequenceMatcher
def get_similarity_score(s1, s2):
    return SequenceMatcher(None, s1, s2).ratio()

def find_similarity(tender_path):
    # Assuming you have Excel files with columns named 'Policy' and 'Rules'
    # df_database1 = pd.read_csv(r'rules_1.csv')
    df_database2 = pd.read_csv(r'rules_2_Power_ministry.csv')
    df_database2.dropna()
    tender_extract, num_pages = extract_text_from_pdf(tender_path)
    embeddings_tender = list(Summarize_doc(tender_extract, num_pages))
    # tender_summary = list(tender_summary)
    
    print("[DEBUG] NUMBER OF TENDER RULES", len(embeddings_tender))
    
    similarity_scores = []
    for policy_db1 in embeddings_tender:
    # Calculate similarity scores for the current policy against all rules in the first database
        # scores_policy = []
        # for policy_db1 in embeddings_tender:
        #     policy_db1 = str(policy_db1)
        #     # policy_db1 = policy_db1.text
        #     # print(type(policy_db1), "[DEBUD}", type(policy_db2))
        #     scores_policy.append(get_similarity_score(policy_db2, policy_db1))
        # return
        scores_for_policy = [get_similarity_score(policy_db2, str(policy_db1)) for policy_db2 in df_database2['Rules']]
    
        # Take the maximum similarity score for the current policy
        max_score = max(scores_for_policy)
        
        # Append the maximum similarity score to the list
        similarity_scores.append(max_score)

    # print("similarity score", similarity_scores)
    scores_map = {}
    for i, value in enumerate(similarity_scores, start=1):
        scores_map[f"Policy {i}"] = f"{value:.8f}" 
        # print(f"Policy {i} : {value:.8f}")
    return scores_map
    # print("[DEBUGGG]", type(tender_summary))
    # for index, rule in enumerate(df_database2['Rules']):
    # # Drop NaN values for each rule
    #     df_database2['Rules'][index] = rule.dropna()
    # for rule in df_database2['Rules']:
    #     # rule.dropna()
    #     print('[DEBUGGGGG]', rule, type(rule))
    
    # print(tender_extract)
    
    # Combine 'Rules' from both databases for f itting the vectorizer
    # all_rules = df_database1['Rules'].tolist() + df_database2['Rules'].tolist()
    all_rules =  df_database2['Rules'].tolist()
    
    # print(all_rules)
    with open("test.txt", "w", encoding='utf-8') as file:
        file.write(str(all_rules))
    
    # Create and fit TF-IDF vectorizer
    vectorizer = TfidfVectorizer()
    vectorizer.fit(all_rules)

    # print("[DEBUG] RULES DB", df_database2)
    # Encode 'Rules' using the same vectorizer
    # embeddings_database1 = encode_sentences(df_database1['Rules'].tolist(), vectorizer)
    embeddings_database2 = encode_sentences(df_database2['Rules'].tolist(), vectorizer)
    print("debug tender type", type(embeddings_tender))
    print("debug type of embedding", type(embeddings_database2))
    # embeddings_tender = encode_sentences(list(tender_summary), vectorizer)
    # Compare policy embeddings with 'Rules' from the second database
    
    similarity_matrix = compare_policy_rule_embeddings(embeddings_tender, embeddings_database2)

    # Calculate average similarity for each policy against all rules in the second database
    average_similarity_per_policy = np.mean(similarity_matrix, axis=1)

    # Create a DataFrame with Policy and its corresponding average similarity
    # result_df = pd.DataFrame({'Policy': df_database1['Policy'], 'Average_Similarity': average_similarity_per_policy})

    # Display the result DataFrame
    print("Policy - Average Similarity:")
    # print(result_df)

# if __name__ == "__main__":
#     main()

# tender_path = r'C:\Users\progg\Desktop\desktop_p\DocuDeck\Tenders\ePublishName.pdf'
# find_similarity(tender_path)